{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ../python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"../Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: 0.09500000160187483\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! \n",
    "\n",
    "#### 4.1 DDPG and MADDPG Algorithms\n",
    "\n",
    "In this project, I use the DDPG algorithm (Deep Deterministic Policy Gradient) and the MADDPG algorithm,\n",
    "a wrapper for DDPG. MADDPG stands for Multi-Agent DDPG. DDPG is an algorithm which concurrently learns\n",
    "a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function, and uses\n",
    "the Q-function to learn the policy. This dual mechanism is the actor-critic method. The DDPG algorithm uses\n",
    "two additional mechanisms: Replay Buffer and Soft Updates.\n",
    "\n",
    "In MADDPG, we train two separate agents, and the agents need to collaborate (like don’t let the ball hit the ground) and compete (like gather as many points as possible). Each agent’s critic is trained using the observations and actions from both agents , whereas each agent’s actor is trained using just its own observations.\n",
    "\n",
    "#### 4.2 Training\n",
    "\n",
    "After some experimentation, finally I have used the following hyperparameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 2e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "EPSILON = 1.0           # for epsilon in the noise process (act step)\n",
    "EPSILON_DECAY = 1e-6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: config==0.4.2 in /home/linuxadmin/.conda/envs/drlnd/lib/python3.6/site-packages (0.4.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install config==0.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import Config\n",
    "from maddpg_agent import MADDPG\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config settings\n",
    "config = Config()\n",
    "config.update_every = 2\n",
    "config.batch_size = 256\n",
    "config.buffer_size = int(1e6)\n",
    "config.discount = 0.99\n",
    "config.tau = 2e-3\n",
    "config.seed = 0\n",
    "config.lr_actor = 1e-3\n",
    "config.lr_critic = 1e-3\n",
    "config.action_size = action_size\n",
    "config.state_size = state_size\n",
    "config.num_agents = num_agents\n",
    "ma = MADDPG(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episode=30000):\n",
    "    \"\"\"\n",
    "    Function to train the agent\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for i_episode in range(n_episode):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        ma.reset()\n",
    "        score = np.zeros(num_agents)\n",
    "        while True:\n",
    "            actions = ma.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            ma.step(states, actions, rewards, next_states, dones)\n",
    "            score += rewards\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        max_score = np.max(score)\n",
    "        scores_window.append(max_score)\n",
    "        scores.append(max_score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} Score: {:.2f} Critic Loss: {:-11.6f} Actor Loss: {:-10.4f}'.\n",
    "              format(i_episode, np.mean(scores_window), max_score, ma.loss[0], ma.loss[1]),\n",
    "              end=\"\")\n",
    "        # periodic model checkpoint\n",
    "        if i_episode % 50 == 0:\n",
    "            torch.save(ma.agents[0].actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(ma.agents[0].critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} Critic Loss: {:-11.6f} Actor Loss: {:-10.4f} '\n",
    "                  't_step {:-8d}'.\n",
    "                  format(i_episode, np.mean(scores_window), ma.loss[0], ma.loss[1], ma.t_step))\n",
    "        # Stopping the training after the avg score of 30 is reached\n",
    "        if np.mean(scores_window) >= 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode,\n",
    "                                                                                         np.mean(scores_window)))\n",
    "            torch.save(ma.agents[0].actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(ma.agents[0].critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: 0.00 Critic Loss:    0.000000 Actor Loss:     0.0000 t_step       14\n",
      "Episode 50\tAverage Score: 0.01 Critic Loss:    0.000021 Actor Loss:    -0.0526 t_step      787\n",
      "Episode 100\tAverage Score: 0.00 Critic Loss:    0.000013 Actor Loss:    -0.0459 t_step     1497\n",
      "Episode 150\tAverage Score: 0.00 Critic Loss:    0.000013 Actor Loss:    -0.0459 t_step     2252\n",
      "Episode 200\tAverage Score: 0.00 Critic Loss:    0.000011 Actor Loss:    -0.0461 t_step     3002\n",
      "Episode 250\tAverage Score: 0.00 Critic Loss:    0.000007 Actor Loss:    -0.0432 t_step     3711\n",
      "Episode 300\tAverage Score: 0.00 Critic Loss:    0.000009 Actor Loss:    -0.0385 t_step     4421\n",
      "Episode 350\tAverage Score: 0.01 Critic Loss:    0.000014 Actor Loss:    -0.0343 t_step     5244\n",
      "Episode 400\tAverage Score: 0.01 Critic Loss:    0.000022 Actor Loss:    -0.0336 t_step     5992\n",
      "Episode 450\tAverage Score: 0.00 Critic Loss:    0.000015 Actor Loss:    -0.0314 t_step     6714\n",
      "Episode 500\tAverage Score: 0.02 Critic Loss:    0.000037 Actor Loss:    -0.0302 t_step     7856\n",
      "Episode 550\tAverage Score: 0.03 Critic Loss:    0.000028 Actor Loss:    -0.0313 t_step     8771\n",
      "Episode 600\tAverage Score: 0.01 Critic Loss:    0.000050 Actor Loss:    -0.0296 t_step     9677\n",
      "Episode 650\tAverage Score: 0.03 Critic Loss:    0.000053 Actor Loss:    -0.0307 t_step    10972\n",
      "Episode 700\tAverage Score: 0.07 Critic Loss:    0.000051 Actor Loss:    -0.0330 t_step    12709\n",
      "Episode 750\tAverage Score: 0.10 Critic Loss:    0.000092 Actor Loss:    -0.0414 t_step    15024\n",
      "Episode 800\tAverage Score: 0.17 Critic Loss:    0.000086 Actor Loss:    -0.0545 t_step    19569\n",
      "Episode 850\tAverage Score: 0.34 Critic Loss:    0.000098 Actor Loss:    -0.0737 t_step    28541\n",
      "Episode 862\tAverage Score: 0.52 Score: 2.60 Critic Loss:    0.000110 Actor Loss:    -0.0887\n",
      "Environment solved in 862 episodes!\tAverage Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "scores = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Training plot\n",
    "The agent was able to solve the environment by achieving score of 30.0 over 100 consecutive episodes after 165 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Future ideas to improve the agent's performance\n",
    "\n",
    "* DDPG can be improved by using prioritized experience replay. \n",
    "* Fine-tuning of hperparameters can also lead to better results and faster training time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
